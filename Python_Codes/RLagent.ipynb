{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# import cupy as cp\n",
    "import itertools\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "class rlagent():\n",
    "    def __init__(self, alpha, gamma, epsilon, cRange, mRange, crossover, mutation, fitness, problem, run):\n",
    "        \n",
    "        # the learning rate\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # the discount rate\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # the exploration rate\n",
    "        self.epsilon = epsilon\n",
    "                                   \n",
    "        self.actionSpace = list(itertools.product(cRange, mRange))\n",
    "        # get the rewards table\n",
    "        self.rewardSpace = np.array([200,   150,   100,   50,    25,\n",
    "                                     150,   113,   75,    38,    19,\n",
    "                                     100,   75,    50,    25,    13,\n",
    "                                     50,    38,    25,    113,   7,\n",
    "                                     0,     0,    -10,   -20,   -30,\n",
    "                                    -1000, -1500, -2000, -2500, -3000])\n",
    "       \n",
    "        # a dictionary of all possible states, where the state is the key, and the value is the index for the q table and rewards space\n",
    "        self.stateSpace = { '(VHC, VHD)': 0, '(VHC, HD)':1,  '(VHC, MD)':2,  '(VHC, LD)':3,  '(VHC, VLD)':4,\n",
    "                            '(HC, VHD)':5,   '(HC, HD)':6,   '(HC, MD)':7,   '(HC, LD)':8,   '(HC, VLD)':9,\n",
    "                            '(LC, VHD)':10,  '(LC, HD)':11,  '(LC, MD)':12,  '(LC, LD)':13,  '(LC, VLD)':14,\n",
    "                            '(VLC, VHD)':15, '(VLC, HD)':16, '(VLC, MD)':17, '(VLC, LD)':18, '(VLC, VLD)':19,\n",
    "                            '(S, VHD)':20,  '(S, HD)':21,  '(S, MD)':22,  '(S, LD)':23,  '(S, VLD)':24,\n",
    "                            '(I, VHD)':25,  '(I, HD)':26,  '(I, MD)':27,  '(I, LD)':28,  '(I, VLD)':29}\n",
    "\n",
    "        self.Q = np.zeros([len(self.stateSpace), len(self.actionSpace)])\n",
    "        \n",
    "        \n",
    "        # ----------------------------------------------- initialization  ------------------------------------------------\n",
    "        # a variable keeping track of how much rewards it has recieved\n",
    "        self.collected  = 0\n",
    "\n",
    "        # create an array to keep count how often each action was taken\n",
    "        self.actionCount = np.zeros(len(self.actionSpace))\n",
    "\n",
    "        # the previous fitness variable is initilized with a verh high cost\n",
    "        self.prevFitness = fitness\n",
    "        \n",
    "        # the current fitness delta\n",
    "        self.fitness = 0\n",
    "\n",
    "        # the current diversity index\n",
    "        self.diversity = 1\n",
    "        \n",
    "        # the current reward awarded\n",
    "        self.reward = 0\n",
    "\n",
    "        # initialize the first state (high cost, and very high diversity)\n",
    "        self.currState = 0\n",
    "\n",
    "        # the first actions are given\n",
    "        self.action = self.actionSpace.index((crossover, mutation))\n",
    "    \n",
    "    \n",
    "    def __max(self, out, arr):\n",
    "        # hold any ties found\n",
    "        ties = []\n",
    "\n",
    "        # set an initial top value\n",
    "        top = float('-inf')\n",
    "\n",
    "        # for each element in the array\n",
    "        for i in range(len(arr)):\n",
    "\n",
    "            # if the current value is the new highest value\n",
    "            if arr[i] > top:\n",
    "\n",
    "                # then reset the tie list\n",
    "                ties = []\n",
    "\n",
    "                # set the new top value\n",
    "                top = arr[i]\n",
    "\n",
    "                # add the top value to the tie list\n",
    "                ties.append([i, arr[i]])\n",
    "\n",
    "            # else if the current value is tied to the highest value\n",
    "            elif arr[i] == top:\n",
    "\n",
    "                # then add it to the tie list\n",
    "                ties.append([arr[i], i])\n",
    "        \n",
    "        # pick a random index\n",
    "        choice = np.random.choice(np.arange(len(ties)))\n",
    "\n",
    "        # return the desired value\n",
    "        return ties[choice][out]\n",
    "    \n",
    "       \n",
    "    def __d_fitness(self, fitnesses):\n",
    "        # get the min fitness of the population\n",
    "        bestFitness = np.amin(fitnesses)\n",
    "        # obtaint the difference between the current and previous fitness values\n",
    "        delta = self.prevFitness - bestFitness\n",
    "        \n",
    "        # the difference is divided by the previous fitness to obtain a percentage\n",
    "        deltaFitness = delta / self.prevFitness\n",
    "        \n",
    "        # the current fitness is set as the previous fitness for the next iteration\n",
    "        self.prevFitness = bestFitness\n",
    "\n",
    "        # return the fitness imrpovement as a percenetage\n",
    "        return deltaFitness\n",
    "    \n",
    "    \n",
    "    def __diversity(population):\n",
    "        sortarr = population[np.lexsort(population.T[::-1])]\n",
    "        mask = np.empty(population.shape[0], dtype=np.bool_)\n",
    "        mask[0] = True\n",
    "        mask[1:] = np.any(sortarr[1:] != sortarr[:-1], axis=1)\n",
    "        diversity = sortarr[mask].shape[0]/population.shape[0]\n",
    "        return diversity\n",
    "\n",
    "    def __reward(self):\n",
    "        # the reward is look up in the table\n",
    "        self.reward = self.rewardSpace[self.nextState]\n",
    "\n",
    "        # the rewards is added to the collection\n",
    "        self.collected += self.reward\n",
    "\n",
    "    # used for printing output\n",
    "    def __findState(self):\n",
    "        for i in self.stateSpace:\n",
    "            if self.stateSpace[i] == self.currState:\n",
    "                return i\n",
    "    \n",
    "    \n",
    "    def __state(self, df, di):\n",
    "        if df < 0:\n",
    "            fState = 'I'\n",
    "        elif df == 0:\n",
    "            fState = 'S'\n",
    "        elif df < 0.01:\n",
    "            fState = 'LL'\n",
    "        elif df < 0.05:\n",
    "            fState = 'L'\n",
    "        elif df < 0.25:\n",
    "            fState = 'H'\n",
    "        else:\n",
    "            fState = 'HH'\n",
    "\n",
    "        # an if statment to convert numerical values into into categorical bins\n",
    "        if di <= 0.2:\n",
    "            dState = 'VLD'\n",
    "        elif di <= 0.4:\n",
    "            dState = 'LD'\n",
    "        elif di <= 0.6:\n",
    "            dState = 'MD'\n",
    "        elif di <= 0.8:\n",
    "            dState = 'HD'\n",
    "        else:\n",
    "            dState = 'VHD'\n",
    "            \n",
    "        state = '(' + fState + ', ' + dState + ')'\n",
    "        self.nextState = self.stateSpace[state]\n",
    "        self.reward = self.__rewardSpace[self.nextState]\n",
    "        \n",
    "        \n",
    "    def initAction(self):\n",
    "        # reset the action count to disregard the first action\n",
    "        self.actionCount = np.zeros(len(self.actionSpace))\n",
    "\n",
    "        # the action count is updated\n",
    "        self.actionCount[self.action] += 1\n",
    "        \n",
    "        # update the results log\n",
    "        # self.__results(0)\n",
    "\n",
    "        # give the enviroment its action (the crossover and mutation probability)\n",
    "        return self.actionSpace[self.action][0], self.actionSpace[self.action][1]\n",
    "\n",
    "    def decide(self, count):\n",
    "        # randomly decide to explore (with probability epsilon)\n",
    "        if np.random.random() <= self.epsilon:\n",
    "\n",
    "            # a random action is chosen\n",
    "            self.action = int(np.random.randint(low=0, high=len(self.actionSpace)))\n",
    "\n",
    "        # or exploit (with probability 1 - epsilon)\n",
    "        else:\n",
    "\n",
    "            # the max action is chosen\n",
    "            self.action = int(self.__max(0, (self.Q[self.currState])))\n",
    "        \n",
    "        # the action count is updated\n",
    "        self.actionCount[self.action] += 1\n",
    "\n",
    "        # print and save the results\n",
    "        # self.__results(count)\n",
    "\n",
    "        # give the enviroment its action (the crossover and mutation probability)\n",
    "        return self.actionSpace[self.action][0], self.actionSpace[self.action][1]\n",
    "        \n",
    "        \n",
    "    # the agent observes the enviroment's response to the agent's action\n",
    "    def observe(self, population, fitnesses):\n",
    "        # obtain the population and their fitnesses after an action\n",
    "        \n",
    "        # determine the delta of the previous fitness and the current best fitness of the population and the diversity \n",
    "        self.fitness = self.__d_fitness(fitnesses)\n",
    "        self.diversity = self.__diversity(population).shape[0]/population.shape[0]\n",
    "        \n",
    "        # get the new state and rewards\n",
    "        self.__state(self.fitness, self.diversity)\n",
    "        self.__reward()\n",
    "\n",
    "    # the Q table is updated along with other variables for the q learning algorithm\n",
    "    def updateQlearning(self):\n",
    "        # update the q table using the bellman equation\n",
    "        self.Q[self.currState, self.action] += self.alpha * (self.reward + self.gamma * self.__max(1, self.Q[self.nextState]) - self.Q[self.currState, self.action] )\n",
    "\n",
    "        # update the current state\n",
    "        self.currState = self.nextState\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "# cRange = np.array(range(1, 10))/10\n",
    "# mRange = np.array(range(1, 10))/10\n",
    "# alpha = 0.7\n",
    "# gamma = 0.1\n",
    "# epsilon = 0.3\n",
    "# crossover = 0.8\n",
    "# mutation = 0.2\n",
    "# agent = rlagent(alpha, gamma, epsilon, cRange, mRange, crossover, mutation)\n",
    "\n",
    "# agent = rlagent()\n",
    "# print(agent.actionSpace[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
